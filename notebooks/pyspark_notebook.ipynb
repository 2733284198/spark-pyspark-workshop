{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "print(\"# schema of both tables\")\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n",
    "\n",
    "print(\"# sample results from both tables\")\n",
    "sales_df.show()\n",
    "item_prices_df.show()\n",
    "\n",
    "print(\"# query execution plan for this simple select\")\n",
    "sales_df.explain()\n",
    "item_prices_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SQL support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select item_id,transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex1. Using plain SQL query select all transactions with quantity = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex2. get mean unit price for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop1_transactions_df = sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\n",
    "\n",
    "print(\"# The same query as above using dataframe api\")\n",
    "shop1_transactions_df.show()\n",
    "\n",
    "\n",
    "sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\\\n",
    "    .show()\n",
    "\n",
    "print(\"# Execution plan of a more complex query\")\n",
    "\n",
    "shop1_transactions_df.explain()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex3. rewrite query from ex1 to dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# joins using plain SQL queries\")\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "print(\"# using Dataframe API - duplicated item_id column!\")\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "print(\"# dropping redundant column\")\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex4. Filter out excluded items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Dataframe with column of items we would like to exclude\")\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "excluded_items_df.show()\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "\n",
    "print(\"# Added new total_sales column which is a multuply of unit_price and qty\")\n",
    "total_sales_df.show()\n",
    "\n",
    "print(\"# Adding price category column based on a condition\")\n",
    "sales_with_transaction_category = total_sales_df\\\n",
    "    .withColumn(\"price_category\", \\\n",
    "                f.when(f.col(\"total_sales\") > 150, \"High\")\\\n",
    "                .when(f.col(\"total_sales\") < 60, \"Low\")\\\n",
    "                .otherwise(\"Medium\"))\n",
    "\n",
    "sales_with_transaction_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex5. We want to create two-packs of items, but their price must be lower than 360, choose those items.\n",
    "hint: use cross join, and alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# aggregate sales by shop - ugly column name\")\n",
    "total_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(total_sales_df.total_sales))\\\n",
    "    .orderBy(f.col(\"sum(total_sales)\")).show()\n",
    "    \n",
    "print(\"# using alias to have a better column name\")\n",
    "total_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(total_sales_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(sales_df.sales) won't work as sales_with_prices has no price column (we define it later)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex6. produce a list of all shops where each item was sold, new column should be named \"shops\"\n",
    "hint: collect_list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Date handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# extracting multiple elements of date\")\n",
    "total_sales_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_string\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()\n",
    "    \n",
    "\n",
    "print(\"# aggregate sales by week\")\n",
    "total_sales_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex7. Weekly sales aggregation not starting on Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using results of one query in another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Calculate global max date\")\n",
    "total_sales_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "print(\"# Let's add it to every column using collect - calling an action\")\n",
    "# 1. using collect/first\n",
    "max_date = total_sales_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .first()[0] #first returns first row, collect returns list of rows\n",
    "    #.collect()[0][0]\n",
    "\n",
    "print(max_date)\n",
    "    \n",
    "print(\"# adding it as a literal (constant) column\")\n",
    "sales_with_max_global_date_df = total_sales_df\\\n",
    "    .withColumn(\"global_max_date\", f.lit(max_date))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex8. using crossJoin (doesn't require invoking action - collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# get max transaction date for each shop using simple aggregations\")\n",
    "max_date_by_store_df = total_sales_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "total_sales_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show()\n",
    "print('# careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings.\\\n",
    "There\\'s no need to drop column')\n",
    "\n",
    "print(\"# another option is to use Windows\")\n",
    "print(\"# Note: Windows are experimental feature (even though they're available since Spark 1.4)\")\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n",
    "    \n",
    "print(\"# Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\")\n",
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()\n",
    "    \n",
    "print(\"# Find average of prices from last two transactions in given shop ordered by transaction date\")\n",
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex9. Find average of prices from current and all previous transactions in given shop ordered by transaction date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Complex aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# produce weekly sales: one row per shop and a list of all transactions \\\n",
    "with week and year numbers for given store in one column \")\n",
    "\n",
    "weekly_sales_by_shop_df = total_sales_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "\n",
    "print(\"# adding week and year columnns\")\n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "print(\"# aggregating sales with three collect_list invocations\")\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "print(\"# Solution above won't work as ordering in each column may be different\")\n",
    "    \n",
    "# shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "#     .groupBy(\"shop_id\")\\\n",
    "#     .agg(f.collect_list([\"sales\", \"week\"]))\n",
    "# won't work, can't collect more than one column\n",
    "\n",
    "print(\"# Using struct inside collect_list solves the problem\")\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "\n",
    "print(\"# What about sorting?\")\n",
    "print(\"# we could do it before aggregation:\")\n",
    "\n",
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "  \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "print(\"# And then use collect_list aggregation\")\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)\n",
    "print(\"# But it won't work, because collect_list may not preserve ordering!\")\n",
    "\n",
    "print(\"# We need to sort it for every row - and to do that we need UDFs - User Defined Functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Defining custom UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_function(column1):\n",
    "    return \"AFTER_UDF_\" + str(column1)\n",
    "\n",
    "my_custom_udf = f.udf(my_custom_function)\n",
    "print(\"# Adding new column by appending string to another one\")\n",
    "df_after_udf = shop_sales_weekly_series_df.withColumn(\"sales_ts_after_udf\", my_custom_udf(f.col(\"sales_ts\")))\n",
    "df_after_udf.show()\n",
    "print(\"# Schema of the new dataframe\")\n",
    "df_after_udf.printSchema()\n",
    "\n",
    "print(\"# We can register our UDF in catalog and use it in SQL query\")\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext.registerFunction(\"my_udf\", my_custom_function)\n",
    "\n",
    "spark.sql(\"select my_udf(shop_id) from sales\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex10. Create your own UDF calculating sales for given transaction by multiplying qty and unit_price  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, ArrayType, StructField\n",
    "\n",
    "\n",
    "print(\"# Returning more than one value from UDF without providing result schema\")\n",
    "def split_shop_id(shop_id):\n",
    "    s, i = shop_id.split(\"_\")\n",
    "    return s, int(i) #must be cast to int, otherwise will return null\n",
    "\n",
    "split_shop_id_udf = f.udf(split_shop_id)\n",
    "df_udf_no_schema = shop_sales_weekly_series_df.withColumn(\"shop_id_splits\", split_shop_id_udf(f.col(\"shop_id\")))\n",
    "print(\"# Results not as expected - seems like calling toString on object\")\n",
    "df_udf_no_schema.show(truncate=False)\n",
    "\n",
    "print(\"# Actual inferred schema: one string instead of a tuple\")\n",
    "df_udf_no_schema.printSchema()\n",
    "\n",
    "print(\"# Defining correct schema with two fields\")\n",
    "schema = StructType([StructField(\"s\", StringType()), StructField(\"i\", IntegerType())])\n",
    "udf_with_schema = f.udf(split_shop_id, schema)\n",
    "\n",
    "df = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", udf_with_schema(f.col(\"shop_id\")))\n",
    "df.show(truncate=False)\n",
    "print(\"# Actual schema is correct as well\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating multiple columns based on a result from UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Extracting all fields from returned struct can be done using asterisk *\")\n",
    "df_split_shop_id = df.select(\"*\", \"shop_id_splits_with_schema.*\").drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id.show()\n",
    "print(\"# Schema was updated and new fields have correct types\")\n",
    "df_split_shop_id.printSchema()\n",
    "\n",
    "print(\"# Solution above will invoke UDF as many times a there are new columns created - \\\n",
    "it's a pySpark behaviour https://issues.apache.org/jira/browse/SPARK-17728\")\n",
    "print(\"# for costly UDF (and in pySpark most of them are very costly) we have a workaround \\\n",
    "to explode an array with one element - result of the UDF\")\n",
    "df_split_shop_id_correct = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", \\\n",
    "                                 f.explode(f.array(udf_with_schema(f.col(\"shop_id\")))))\n",
    "\n",
    "df_split_shop_id_correct = df_split_shop_id_correct \\\n",
    "    .select(\"*\", \"shop_id_splits_with_schema.*\") \\\n",
    "    .drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id_correct.show()\n",
    "print(\"# Results and schema are the same\")\n",
    "df_split_shop_id_correct.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Identifying problems with UDFs\")\n",
    "    \n",
    "print(\"# To see why Spark invokes UDF multiple times let's look at query execution plan\")\n",
    "print(\"# For the first version we can see: \")\n",
    "print(\"# +- BatchEvalPython [split_shop_id(shop_id#238), split_shop_id(shop_id#238), split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, pythonUDF0#5025, pythonUDF1#5026, pythonUDF2#5027]\")\n",
    "print(\"# which contains multiple pythonUDF references\")\n",
    "print(\"\")\n",
    "print(\"# For the updated solution there's only one invocation: \")\n",
    "print(\"# +- BatchEvalPython [split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, shop_id_splits#4556, pythonUDF0#5031]\")\n",
    "print(\"\")\n",
    "df_split_shop_id.explain()\n",
    "print(\"\")\n",
    "df_split_shop_id_correct.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.11 sort each time series from previous part in descending order and compare to initial ts (tip: use sorted method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
