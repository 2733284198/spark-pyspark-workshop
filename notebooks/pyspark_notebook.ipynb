{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created local SparkSession\n",
      "Created \"sales\" view from CSV file\n",
      "Created \"item_prices\" view from CSV file\n"
     ]
    }
   ],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- qty: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|item_id|unit_price|\n",
      "+-------+----------+\n",
      "| ITEM_1|     100.0|\n",
      "| ITEM_2|     300.0|\n",
      "| ITEM_3|      50.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO rename sales_with_prices_df to sales_df\n",
    "\n",
    "# 0. ENV PREPARATION\n",
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n",
    "\n",
    "sales_df.show()\n",
    "item_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------------------------------+\n",
      "|avg(CAST(unit_price AS DOUBLE))|\n",
      "+-------------------------------+\n",
      "|                          150.0|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. SQL support\n",
    "spark.sql('select item_id,transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\\\n",
    "    .show()\n",
    "\n",
    "# ex1. Using plain SQL query select all transactions for with quantity = 1\n",
    "spark.sql('select * from sales where qty = 1').show()\n",
    "# ex2. get mean unit price for all items\n",
    "spark.sql('select mean(unit_price) from item_prices').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2. Dataframe operations\n",
    "sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\\\n",
    "    .show()\n",
    "    \n",
    "sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "#ex3. rewrite query from ex1 to dataframe operations\n",
    "sales_df.filter(f.col(\"qty\") == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+---+----------------+-------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+---+----------------+-------+----------+\n",
      "\n",
      "+------+\n",
      "|  item|\n",
      "+------+\n",
      "|ITEM_2|\n",
      "|ITEM_4|\n",
      "+------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. JOINS\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()\n",
    "\n",
    "#ex4. Filter out excluded items\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "\n",
    "excluded_items_df.show()\n",
    "\n",
    "# using join and filtering\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_outer\")\\\n",
    "    .filter(f.isnull(excluded_items_df.item))\\\n",
    "    .drop(excluded_items_df.item)\\\n",
    "    .show()\n",
    "    \n",
    "# better option: anti join\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_anti\")\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|price_category|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|          High|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|          High|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|          High|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|           Low|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|        Medium|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "\n",
      "+-------+-------+---------+\n",
      "|item_id|item_id|price_sum|\n",
      "+-------+-------+---------+\n",
      "| ITEM_1| ITEM_3|    150.0|\n",
      "| ITEM_2| ITEM_3|    350.0|\n",
      "| ITEM_3| ITEM_1|    150.0|\n",
      "| ITEM_3| ITEM_2|    350.0|\n",
      "+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Adding columns\n",
    "sales_with_prices_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "    \n",
    "sales_with_prices_df.show()\n",
    "\n",
    "# Adding column based on a condition\n",
    "sales_with_transaction_category = sales_with_prices_df\\\n",
    "    .withColumn(\"price_category\", \\\n",
    "                f.when(f.col(\"total_sales\") > 150, \"High\")\\\n",
    "                .when(f.col(\"total_sales\") < 60, \"Low\")\\\n",
    "                .otherwise(\"Medium\"))\n",
    "\n",
    "sales_with_transaction_category.show()\n",
    "\n",
    "\n",
    "#ex5. We want to create two-packs of items, but their price must be lower than 360, choose those items.\n",
    "#hint: use cross join, and alias\n",
    "item_prices_df.alias(\"items1\")\\\n",
    "    .crossJoin(item_prices_df.alias(\"items2\"))\\\n",
    "    .withColumn(\"price_sum\",f.col(\"items1.unit_price\") + f.col(\"items2.unit_price\"))\\\n",
    "    .where((f.col(\"price_sum\") < 360) & (f.col(\"items1.item_id\") != f.col(\"items2.item_id\")))\\\n",
    "    .select(\"items1.item_id\", \"items2.item_id\", \"price_sum\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|shop_id|sum(total_sales)|\n",
      "+-------+----------------+\n",
      "| SHOP_2|           150.0|\n",
      "| SHOP_1|           700.0|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|shop_id|sales|\n",
      "+-------+-----+\n",
      "| SHOP_1|700.0|\n",
      "| SHOP_2|150.0|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+----------------+\n",
      "|item_id|           shops|\n",
      "+-------+----------------+\n",
      "| ITEM_3|[SHOP_1, SHOP_2]|\n",
      "| ITEM_2|        [SHOP_1]|\n",
      "| ITEM_1|[SHOP_1, SHOP_2]|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Simple aggregations\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales))\\\n",
    "    .orderBy(f.col(\"sum(total_sales)\")).show()\n",
    "    \n",
    "#using alias to avoid strange column names\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(sales_with_prices_df.sales) won't work as sales_with_prices has no price column (we define it later)\n",
    "    \n",
    "# ex6. produce a list of all shops where each item was sold, new column should be named \"shops\"\n",
    "# hint: collect_list function\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"item_id\")\\\n",
    "    .agg(f.collect_list(f.col(\"shop_id\")).alias(\"shops\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|year|month|day|day_of_year|day_of_week|day_of_week_string|week_of_year|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018|    2| 10|         41|          6|               Sat|           6|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018|    2|  2|         33|          5|               Fri|           5|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018|    2| 11|         42|          7|               Sun|           6|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "\n",
      "+----------------------------+----------------+\n",
      "|weekofyear(transaction_date)|sum(total_sales)|\n",
      "+----------------------------+----------------+\n",
      "|                           6|           300.0|\n",
      "|                           5|           550.0|\n",
      "+----------------------------+----------------+\n",
      "\n",
      "+----------------------------------+----------------+\n",
      "|weekofyear(transaction_date_moved)|sum(total_sales)|\n",
      "+----------------------------------+----------------+\n",
      "|                                 6|           200.0|\n",
      "|                                 5|           550.0|\n",
      "|                                 7|           100.0|\n",
      "+----------------------------------+----------------+\n",
      "\n",
      "+----------+----------------+------------------+\n",
      "| aggr_date|sum(total_sales)|day_of_week_string|\n",
      "+----------+----------------+------------------+\n",
      "|2018-02-10|           200.0|               Sat|\n",
      "|2018-02-17|           100.0|               Sat|\n",
      "|2018-02-03|           550.0|               Sat|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Date handling\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_string\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()\n",
    "    \n",
    "\n",
    "# aggregate sales by week\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "# ex7. Weekly sales aggregation not starting on Monday\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"transaction_date_moved\", f.date_add(f.col(\"transaction_date\"), 1))\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date_moved\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "# Unfortunately week_of_year column will have incorrect values (shifted by 1 day)\n",
    "# but that's not a problem for calculations that require only ordering\n",
    "\n",
    "# Different solution where we preserve last day of every week \n",
    "#\"Sat\" can be seen as a day where week ends\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"aggr_date\", f.next_day(f.date_sub(f.col(\"transaction_date\"), 1), \"Sat\"))\\\n",
    "    .groupBy(f.col(\"aggr_date\"))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .withColumn(\"day_of_week_string\",  f.date_format(f.col(\"aggr_date\"), 'E'))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  max_date|\n",
      "+----------+\n",
      "|2018-02-11|\n",
      "+----------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|global_max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|     2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|     2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|     2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|  max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Using result of one query in another\n",
    "#max date globaly\n",
    "sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#how to add it to every column\n",
    "# 1. using collect/first\n",
    "max_date = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .first()[0] #first returns first row, collect returns list of rows\n",
    "    #.collect()[0][0]\n",
    "\n",
    "sales_with_max_global_date_df = sales_with_prices_df\\\n",
    "    .withColumn(\"global_max_date\", f.lit(max_date))\\\n",
    "    .show()\n",
    "\n",
    "# ex8. \n",
    "# 2. using crossJoin (doesn't require invoking action - collect)\n",
    "max_date_df = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\n",
    "    \n",
    "sales_with_max_global_date_cross_join_df = sales_with_prices_df\\\n",
    "    .crossJoin(f.broadcast(max_date_df))\\\n",
    "    .show()\n",
    "#make sure DF inside cross join has only one element, if not then we'll have too many rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|item_transaction_ordinal|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                       1|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                       2|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                       1|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                       1|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                       2|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|price_moving_average|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|               250.0|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|               200.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|               250.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|average_price_until_now|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  250.0|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  200.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     233.33333333333334|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                   50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                   75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Window functions\n",
    "# get max transaction date for each shop\n",
    "\n",
    "max_date_by_store_df = sales_with_prices_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "sales_with_prices_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show()\n",
    "#careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings\n",
    "#no need to drop column\n",
    "\n",
    "#another option is to use Windows\n",
    "#Note: Windows are experimental feature (even though they're available since Spark 1.4\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n",
    "    \n",
    "#Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\n",
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()\n",
    "    \n",
    "#Find average of prices from last two transactions in given shop ordered by transaction date\n",
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#ex9. Find average of prices from current and all previous transactions in given shop ordered by transaction date\n",
    "unbounded_window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "    \n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"average_price_until_now\", f.mean(f.col(\"total_sales\")).over(unbounded_window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|shop_id|collect_list(week)|collect_list(year)|collect_list(sales)|\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|SHOP_2 |[5, 6]            |[2018, 2018]      |[50.0, 100.0]      |\n",
      "|SHOP_1 |[5, 6]            |[2018, 2018]      |[500.0, 200.0]     |\n",
      "+-------+------------------+------------------+-------------------+\n",
      "\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n",
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Complex aggregations\n",
    "\n",
    "# produce one row per shop and a list of all transactions with week and year numbers for given store in one column\n",
    "\n",
    "# produce weekly sales by shop\n",
    "weekly_sales_by_shop_df = sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "    \n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "#solution above won't work as ordering in each column may be different\n",
    "    \n",
    "# shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "#     .groupBy(\"shop_id\")\\\n",
    "#     .agg(f.collect_list([\"sales\", \"week\"]))\n",
    "# won't work, can't collect more than one column\n",
    "\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "\n",
    "# what about sorting?\n",
    "# we could do it before aggregation:\n",
    "\n",
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "    \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)\n",
    "#it won't work, because collect_list may not preserve ordering!\n",
    "\n",
    "#we need to sort it for evetry row - and to do that we need UDFs - User Defined Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|shop_id|            sales_ts|  sales_ts_after_udf|\n",
      "+-------+--------------------+--------------------+\n",
      "| SHOP_2|[[2018,5,50.0], [...|AFTER_UDF_[Row(ye...|\n",
      "| SHOP_1|[[2018,5,500.0], ...|AFTER_UDF_[Row(ye...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- sales_ts_after_udf: string (nullable = true)\n",
      "\n",
      "+----------------+\n",
      "| my_udf(shop_id)|\n",
      "+----------------+\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "+----------------+\n",
      "\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@1ed324e1|\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@27d204c6|\n",
      "+-------+--------------------------------+----------------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      "\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |shop_id_splits_with_schema|\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@21ee28db|[SHOP,2]                  |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@70ab579a|[SHOP,1]                  |\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- shop_id_splits_with_schema: struct (nullable = true)\n",
      " |    |-- s: string (nullable = true)\n",
      " |    |-- i: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [shop_id#12, sales_ts#892, pythonUDF0#3104 AS shop_id_splits#2893, pythonUDF2#3106.s AS s#2982, pythonUDF2#3106.i AS i#2983]\n",
      "+- BatchEvalPython [split_shop_id(shop_id#12), split_shop_id(shop_id#12), split_shop_id(shop_id#12)], [shop_id#12, sales_ts#892, pythonUDF0#3104, pythonUDF1#3105, pythonUDF2#3106]\n",
      "   +- ObjectHashAggregate(keys=[shop_id#12], functions=[collect_list(named_struct(year, year#747, week, week#746, sales, sales#756), 0, 0)])\n",
      "      +- Exchange hashpartitioning(shop_id#12, 200)\n",
      "         +- ObjectHashAggregate(keys=[shop_id#12], functions=[partial_collect_list(named_struct(year, year#747, week, week#746, sales, sales#756), 0, 0)])\n",
      "            +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date))#3102, year(cast(transaction_date#15 as date))#3103], functions=[sum(total_sales#302)])\n",
      "               +- Exchange hashpartitioning(shop_id#12, weekofyear(cast(transaction_date#15 as date))#3102, year(cast(transaction_date#15 as date))#3103, 200)\n",
      "                  +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date)) AS weekofyear(cast(transaction_date#15 as date))#3102, year(cast(transaction_date#15 as date)) AS year(cast(transaction_date#15 as date))#3103], functions=[partial_sum(total_sales#302)])\n",
      "                     +- *Project [shop_id#12, transaction_date#15, (cast(qty#14 as double) * cast(unit_price#35 as double)) AS total_sales#302]\n",
      "                        +- *BroadcastHashJoin [item_id#13], [item_id#34], Inner, BuildRight\n",
      "                           :- *Project [shop_id#12, item_id#13, qty#14, transaction_date#15]\n",
      "                           :  +- *Filter isnotnull(item_id#13)\n",
      "                           :     +- *FileScan csv [shop_id#12,item_id#13,qty#14,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<shop_id:string,item_id:string,qty:string,transaction_date:string>\n",
      "                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                              +- *Project [item_id#34, unit_price#35]\n",
      "                                 +- *Filter isnotnull(item_id#34)\n",
      "                                    +- *FileScan csv [item_id#34,unit_price#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<item_id:string,unit_price:string>\n",
      "== Physical Plan ==\n",
      "*Project [shop_id#12, sales_ts#892, shop_id_splits#2893, shop_id_splits_with_schema#3040.s AS s#3046, shop_id_splits_with_schema#3040.i AS i#3047]\n",
      "+- *Project [shop_id#12, sales_ts#892, shop_id_splits#2893, shop_id_splits_with_schema#3040]\n",
      "   +- Generate explode(array(pythonUDF0#3110)), true, false, [shop_id_splits_with_schema#3040]\n",
      "      +- BatchEvalPython [split_shop_id(shop_id#12)], [shop_id#12, sales_ts#892, shop_id_splits#2893, pythonUDF0#3110]\n",
      "         +- *Project [shop_id#12, sales_ts#892, pythonUDF0#3109 AS shop_id_splits#2893]\n",
      "            +- BatchEvalPython [split_shop_id(shop_id#12)], [shop_id#12, sales_ts#892, pythonUDF0#3109]\n",
      "               +- ObjectHashAggregate(keys=[shop_id#12], functions=[collect_list(named_struct(year, year#747, week, week#746, sales, sales#756), 0, 0)])\n",
      "                  +- Exchange hashpartitioning(shop_id#12, 200)\n",
      "                     +- ObjectHashAggregate(keys=[shop_id#12], functions=[partial_collect_list(named_struct(year, year#747, week, week#746, sales, sales#756), 0, 0)])\n",
      "                        +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date))#3107, year(cast(transaction_date#15 as date))#3108], functions=[sum(total_sales#302)])\n",
      "                           +- Exchange hashpartitioning(shop_id#12, weekofyear(cast(transaction_date#15 as date))#3107, year(cast(transaction_date#15 as date))#3108, 200)\n",
      "                              +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date)) AS weekofyear(cast(transaction_date#15 as date))#3107, year(cast(transaction_date#15 as date)) AS year(cast(transaction_date#15 as date))#3108], functions=[partial_sum(total_sales#302)])\n",
      "                                 +- *Project [shop_id#12, transaction_date#15, (cast(qty#14 as double) * cast(unit_price#35 as double)) AS total_sales#302]\n",
      "                                    +- *BroadcastHashJoin [item_id#13], [item_id#34], Inner, BuildRight\n",
      "                                       :- *Project [shop_id#12, item_id#13, qty#14, transaction_date#15]\n",
      "                                       :  +- *Filter isnotnull(item_id#13)\n",
      "                                       :     +- *FileScan csv [shop_id#12,item_id#13,qty#14,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<shop_id:string,item_id:string,qty:string,transaction_date:string>\n",
      "                                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                                          +- *Project [item_id#34, unit_price#35]\n",
      "                                             +- *Filter isnotnull(item_id#34)\n",
      "                                                +- *FileScan csv [item_id#34,unit_price#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<item_id:string,unit_price:string>\n",
      "sorted:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|shop_id|            sales_ts|           sorted_ts|\n",
      "+-------+--------------------+--------------------+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[[2018,6,100.0], ...|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[[2018,6,200.0], ...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Defining custom UDFs\n",
    "def my_custom_function(column1):\n",
    "    return \"AFTER_UDF_\" + str(column1)\n",
    "\n",
    "my_custom_udf = f.udf(my_custom_function)\n",
    "\n",
    "df_after_udf = shop_sales_weekly_series_df.withColumn(\"sales_ts_after_udf\", my_custom_udf(f.col(\"sales_ts\")))\n",
    "df_after_udf.show()\n",
    "df_after_udf.printSchema()\n",
    "\n",
    "#we can register our UDF in catalog and use it in SQL query\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext.registerFunction(\"my_udf\", my_custom_function)\n",
    "\n",
    "spark.sql(\"select my_udf(shop_id) from sales\").show()\n",
    "\n",
    "# TODO typed version of UDFs\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, ArrayType, StructField\n",
    "\n",
    "\n",
    "#TODO what happens when string type and we return INT and in other way\n",
    "\n",
    "# returning multiple columns from UDF\n",
    "\n",
    "def split_shop_id(shop_id):\n",
    "    s, i = shop_id.split(\"_\")\n",
    "    return s, int(i) #must be cast to int, otherwise will return null\n",
    "\n",
    "split_shop_id_udf = f.udf(split_shop_id)\n",
    "\n",
    "df_udf_no_schema = shop_sales_weekly_series_df.withColumn(\"shop_id_splits\", split_shop_id_udf(f.col(\"shop_id\")))\n",
    "df_udf_no_schema.show(truncate=False)\n",
    "df_udf_no_schema.printSchema()\n",
    "\n",
    "schema = StructType([StructField(\"s\", StringType()), StructField(\"i\", IntegerType())])\n",
    "udf_with_schema = f.udf(split_shop_id, schema)\n",
    "\n",
    "df = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", udf_with_schema(f.col(\"shop_id\")))\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "#how to get each result element as separate column?\n",
    "df_split_shop_id = df.select(\"*\", \"shop_id_splits_with_schema.*\").drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id.show()\n",
    "df_split_shop_id.printSchema()\n",
    "\n",
    "#Solution above will invoke UDF as many times a there are new columns created - it's a pySpark bug fixed in Spark 2.3\n",
    "# https://issues.apache.org/jira/browse/SPARK-17728\n",
    "#workaround\n",
    "df_split_shop_id_correct = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", \\\n",
    "                                 f.explode(f.array(udf_with_schema(f.col(\"shop_id\")))))\n",
    "\n",
    "df_split_shop_id_correct = df_split_shop_id_correct \\\n",
    "    .select(\"*\", \"shop_id_splits_with_schema.*\") \\\n",
    "    .drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id_correct.show()\n",
    "df_split_shop_id_correct.printSchema()\n",
    "    \n",
    "df_split_shop_id.explain()\n",
    "df_split_shop_id_correct.explain()\n",
    "\n",
    "#ex.10 sort each time series from previous part in descending order and compare to initial ts (tip: use sorted method)\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "def sort_ts(ts):\n",
    "    s_ts = sorted(ts, key=lambda row: (-row.week, -row.year))\n",
    "    return s_ts\n",
    "\n",
    "sort_ts_udf = f.udf(sort_ts, ArrayType(StructType(\n",
    "            [StructField(\"year\", IntegerType()),\n",
    "             StructField(\"week\", IntegerType()),\n",
    "             StructField(\"sales\", FloatType())])))\n",
    "\n",
    "sorted_ts_df = wrongly_sorted_series_df.withColumn(\"sorted_ts\", sort_ts_udf(f.col(\"sales_ts\")))\n",
    "\n",
    "sorted_ts_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
