{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created local SparkSession\n"
     ]
    }
   ],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO rename sales_with_prices_df to sales_df\n",
    "\n",
    "# 0. ENV PREPARATION\n",
    "spark.read.option(\"header\", \"true\").csv(\"/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv\").createOrReplaceTempView(\"sales\")\n",
    "spark.read.option(\"header\", \"true\").csv(\"/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv\").createOrReplaceTempView(\"item_prices\")\n",
    "\n",
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n",
    "\n",
    "sales_df.show()\n",
    "item_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. SQL support\n",
    "spark.sql('select item_id,transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\\\n",
    "    .show()\n",
    "# TODO add other examples to rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Dataframe operations\n",
    "sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\\\n",
    "    .show()\n",
    "    \n",
    "sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. JOINS\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()\n",
    "\n",
    "#Filter out excluded items\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "\n",
    "excluded_items_df.show()\n",
    "\n",
    "# using join and filtering\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_outer\")\\\n",
    "    .filter(f.isnull(excluded_items_df.item))\\\n",
    "    .drop(excluded_items_df.item)\\\n",
    "    .show()\n",
    "    \n",
    "# better option: anti join\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_anti\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Adding columns\n",
    "sales_with_prices_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "    \n",
    "sales_with_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Simple aggregations\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales))\\\n",
    "    .orderBy(f.col(\"sum(total_sales)\")).show()\n",
    "    \n",
    "#using alias to avoid strange column names\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(sales_with_prices_df.sales) won't work as sales_with_prices has no price column (we define it later)\n",
    "    \n",
    "# produce a list of all shops where each item was sold\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"item_id\")\\\n",
    "    .agg(f.collect_list(f.col(\"shop_id\")).alias(\"shops\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6. Date handling\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_string\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()\n",
    "    \n",
    "\n",
    "# aggregate sales by week\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "# Weekly aggregations not starting on Monday    \n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"transaction_date_moved\", f.date_add(f.col(\"transaction_date\"), 1))\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date_moved\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "#problem with incorrect value of the week_of_year, enough for callulations that require ordering\n",
    "\n",
    "# Different solution where we preserve last day of every week \n",
    "#\"Sat\" can be seen as a day where week ends\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"aggr_date\", f.next_day(f.date_sub(f.col(\"transaction_date\"), 1), \"Sat\"))\\\n",
    "    .groupBy(f.col(\"aggr_date\"))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .withColumn(\"day_of_week_string\",  f.date_format(f.col(\"aggr_date\"), 'E'))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7. Using result of one query in another\n",
    "#max date globaly\n",
    "sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#how to add it to every column\n",
    "# 1. using collect/first\n",
    "max_date = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .first()[0] #first returns first row, collect returns list of rows\n",
    "    #.collect()[0][0]\n",
    "\n",
    "sales_with_max_global_date_df = sales_with_prices_df\\\n",
    "    .withColumn(\"global_max_date\", f.lit(max_date))\\\n",
    "    .show()\n",
    "    \n",
    "#2. using crossJoin (doesn't require invoking action)\n",
    "max_date_df = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\n",
    "    \n",
    "sales_with_max_global_date_cross_join_df = sales_with_prices_df\\\n",
    "    .crossJoin(f.broadcast(max_date_df))\\\n",
    "    .show()\n",
    "#make sure DF inside cross join has only one element, if not then we'll have too many rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8. Window functions\n",
    "# get max transaction date for each shop\n",
    "\n",
    "max_date_by_store_df = sales_with_prices_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "sales_with_prices_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show()\n",
    "#careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings\n",
    "#no need to drop column\n",
    "\n",
    "#another option is to use Windows\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n",
    "    \n",
    "#Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\n",
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()\n",
    "    \n",
    "#Find average of prices from last two transactions in given shop ordered by transaction date\n",
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#find average of prices from current and all previous transactions in given shop ordered by transaction date\n",
    "unbounded_window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "    \n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"average_price_until_now\", f.mean(f.col(\"total_sales\")).over(unbounded_window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 9. Complex aggregations\n",
    "\n",
    "# produce one row per shop and a list of all transactions with week and year numbers for given store in one column\n",
    "\n",
    "# produce weekly sales by shop\n",
    "weekly_sales_by_shop_df = sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "    \n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "#solution above won't work as ordering in each column may be different\n",
    "    \n",
    "# shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "#     .groupBy(\"shop_id\")\\\n",
    "#     .agg(f.collect_list([\"sales\", \"week\"]))\n",
    "# won't work, can't collect more than one column\n",
    "\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "\n",
    "# what about sorting?\n",
    "# we could do it before aggregation:\n",
    "\n",
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "    \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)\n",
    "#it won't work, because collect_list may not preserve ordering!\n",
    "\n",
    "#we need to sort it for evetry row - and to do that we need UDFs - User Defined Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10. Defining custom UDFs\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tap_ds_env]",
   "language": "python",
   "name": "conda-env-tap_ds_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
