{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created local SparkSession\n",
      "Created \"sales\" view from CSV file\n",
      "Created \"item_prices\" view from CSV file\n"
     ]
    }
   ],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- qty: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|item_id|unit_price|\n",
      "+-------+----------+\n",
      "| ITEM_1|     100.0|\n",
      "| ITEM_2|     300.0|\n",
      "| ITEM_3|      50.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO rename sales_with_prices_df to sales_df\n",
    "\n",
    "# 0. ENV PREPARATION\n",
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n",
    "\n",
    "sales_df.show()\n",
    "item_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. SQL support\n",
    "spark.sql('select item_id,transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\\\n",
    "    .show()\n",
    "# TODO add other examples to rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2. Dataframe operations\n",
    "sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\\\n",
    "    .show()\n",
    "    \n",
    "sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+---+----------------+-------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+---+----------------+-------+----------+\n",
      "\n",
      "+------+\n",
      "|  item|\n",
      "+------+\n",
      "|ITEM_2|\n",
      "|ITEM_4|\n",
      "+------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. JOINS\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()\n",
    "\n",
    "#Filter out excluded items\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "\n",
    "excluded_items_df.show()\n",
    "\n",
    "# using join and filtering\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_outer\")\\\n",
    "    .filter(f.isnull(excluded_items_df.item))\\\n",
    "    .drop(excluded_items_df.item)\\\n",
    "    .show()\n",
    "    \n",
    "# better option: anti join\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_anti\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Adding columns\n",
    "sales_with_prices_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "    \n",
    "sales_with_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|shop_id|sum(total_sales)|\n",
      "+-------+----------------+\n",
      "| SHOP_2|           150.0|\n",
      "| SHOP_1|           700.0|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|shop_id|sales|\n",
      "+-------+-----+\n",
      "| SHOP_1|700.0|\n",
      "| SHOP_2|150.0|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+----------------+\n",
      "|item_id|           shops|\n",
      "+-------+----------------+\n",
      "| ITEM_3|[SHOP_1, SHOP_2]|\n",
      "| ITEM_2|        [SHOP_1]|\n",
      "| ITEM_1|[SHOP_1, SHOP_2]|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Simple aggregations\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales))\\\n",
    "    .orderBy(f.col(\"sum(total_sales)\")).show()\n",
    "    \n",
    "#using alias to avoid strange column names\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(sales_with_prices_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(sales_with_prices_df.sales) won't work as sales_with_prices has no price column (we define it later)\n",
    "    \n",
    "# produce a list of all shops where each item was sold\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(\"item_id\")\\\n",
    "    .agg(f.collect_list(f.col(\"shop_id\")).alias(\"shops\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|year|month|day|day_of_year|day_of_week|day_of_week_string|week_of_year|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018|    2| 10|         41|          6|               Sat|           6|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018|    2|  2|         33|          5|               Fri|           5|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018|    2| 11|         42|          7|               Sun|           6|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "\n",
      "+----------------------------+----------------+\n",
      "|weekofyear(transaction_date)|sum(total_sales)|\n",
      "+----------------------------+----------------+\n",
      "|                           6|           300.0|\n",
      "|                           5|           550.0|\n",
      "+----------------------------+----------------+\n",
      "\n",
      "+----------------------------------+----------------+\n",
      "|weekofyear(transaction_date_moved)|sum(total_sales)|\n",
      "+----------------------------------+----------------+\n",
      "|                                 6|           200.0|\n",
      "|                                 5|           550.0|\n",
      "|                                 7|           100.0|\n",
      "+----------------------------------+----------------+\n",
      "\n",
      "+----------+----------------+------------------+\n",
      "| aggr_date|sum(total_sales)|day_of_week_string|\n",
      "+----------+----------------+------------------+\n",
      "|2018-02-10|           200.0|               Sat|\n",
      "|2018-02-17|           100.0|               Sat|\n",
      "|2018-02-03|           550.0|               Sat|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Date handling\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_string\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()\n",
    "    \n",
    "\n",
    "# aggregate sales by week\n",
    "sales_with_prices_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "# Weekly aggregations not starting on Monday    \n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"transaction_date_moved\", f.date_add(f.col(\"transaction_date\"), 1))\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date_moved\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "#problem with incorrect value of the week_of_year, enough for callulations that require ordering\n",
    "\n",
    "# Different solution where we preserve last day of every week \n",
    "#\"Sat\" can be seen as a day where week ends\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"aggr_date\", f.next_day(f.date_sub(f.col(\"transaction_date\"), 1), \"Sat\"))\\\n",
    "    .groupBy(f.col(\"aggr_date\"))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .withColumn(\"day_of_week_string\",  f.date_format(f.col(\"aggr_date\"), 'E'))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  max_date|\n",
      "+----------+\n",
      "|2018-02-11|\n",
      "+----------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|global_max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|     2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|     2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|     2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|  max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Using result of one query in another\n",
    "#max date globaly\n",
    "sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#how to add it to every column\n",
    "# 1. using collect/first\n",
    "max_date = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .first()[0] #first returns first row, collect returns list of rows\n",
    "    #.collect()[0][0]\n",
    "\n",
    "sales_with_max_global_date_df = sales_with_prices_df\\\n",
    "    .withColumn(\"global_max_date\", f.lit(max_date))\\\n",
    "    .show()\n",
    "    \n",
    "#2. using crossJoin (doesn't require invoking action)\n",
    "max_date_df = sales_with_prices_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\n",
    "    \n",
    "sales_with_max_global_date_cross_join_df = sales_with_prices_df\\\n",
    "    .crossJoin(f.broadcast(max_date_df))\\\n",
    "    .show()\n",
    "#make sure DF inside cross join has only one element, if not then we'll have too many rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|item_transaction_ordinal|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                       1|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                       2|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                       1|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                       1|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                       2|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|price_moving_average|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|               250.0|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|               200.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|               250.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|average_price_until_now|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  250.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     233.33333333333334|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                   50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                   75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Window functions\n",
    "# get max transaction date for each shop\n",
    "\n",
    "max_date_by_store_df = sales_with_prices_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "sales_with_prices_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show()\n",
    "#careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings\n",
    "#no need to drop column\n",
    "\n",
    "#another option is to use Windows\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n",
    "    \n",
    "#Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\n",
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()\n",
    "    \n",
    "#Find average of prices from last two transactions in given shop ordered by transaction date\n",
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "#find average of prices from current and all previous transactions in given shop ordered by transaction date\n",
    "unbounded_window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "    \n",
    "sales_with_prices_df\\\n",
    "    .withColumn(\"average_price_until_now\", f.mean(f.col(\"total_sales\")).over(unbounded_window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|shop_id|collect_list(week)|collect_list(year)|collect_list(sales)|\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|SHOP_2 |[5, 6]            |[2018, 2018]      |[50.0, 100.0]      |\n",
      "|SHOP_1 |[5, 6]            |[2018, 2018]      |[500.0, 200.0]     |\n",
      "+-------+------------------+------------------+-------------------+\n",
      "\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n",
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Complex aggregations\n",
    "\n",
    "# produce one row per shop and a list of all transactions with week and year numbers for given store in one column\n",
    "\n",
    "# produce weekly sales by shop\n",
    "weekly_sales_by_shop_df = sales_with_prices_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "    \n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "#solution above won't work as ordering in each column may be different\n",
    "    \n",
    "# shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "#     .groupBy(\"shop_id\")\\\n",
    "#     .agg(f.collect_list([\"sales\", \"week\"]))\n",
    "# won't work, can't collect more than one column\n",
    "\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "\n",
    "# what about sorting?\n",
    "# we could do it before aggregation:\n",
    "\n",
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "    \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)\n",
    "#it won't work, because collect_list may not preserve ordering!\n",
    "\n",
    "#we need to sort it for evetry row - and to do that we need UDFs - User Defined Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = \"a_b\".split(\"_\")\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|shop_id|            sales_ts|  sales_ts_after_udf|\n",
      "+-------+--------------------+--------------------+\n",
      "| SHOP_2|[[2018,5,50.0], [...|AFTER_UDF_[Row(ye...|\n",
      "| SHOP_1|[[2018,5,500.0], ...|AFTER_UDF_[Row(ye...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- sales_ts_after_udf: string (nullable = true)\n",
      "\n",
      "+----------------+\n",
      "| my_udf(shop_id)|\n",
      "+----------------+\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "+----------------+\n",
      "\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@60098663|\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@7d45e2e7|\n",
      "+-------+--------------------------------+----------------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      "\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |shop_id_splits_with_schema|\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@167546ab|[SHOP,2]                  |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@6c06b0d0|[SHOP,1]                  |\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- shop_id_splits_with_schema: struct (nullable = true)\n",
      " |    |-- s: string (nullable = true)\n",
      " |    |-- i: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Defining custom UDFs\n",
    "def my_custom_function(column1):\n",
    "    return \"AFTER_UDF_\" + str(column1)\n",
    "\n",
    "my_custom_udf = f.udf(my_custom_function)\n",
    "\n",
    "df_after_udf = shop_sales_weekly_series_df.withColumn(\"sales_ts_after_udf\", my_custom_udf(f.col(\"sales_ts\")))\n",
    "df_after_udf.show()\n",
    "df_after_udf.printSchema()\n",
    "\n",
    "\n",
    "#we can register our UDF in catalog and use it in SQL query\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext.registerFunction(\"my_udf\", my_custom_function)\n",
    "\n",
    "spark.sql(\"select my_udf(shop_id) from sales\").show()\n",
    "\n",
    "# TODO typed version of UDFs\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField\n",
    "\n",
    "\n",
    "#TODO what happens when string type and we return INT and in other way\n",
    "\n",
    "# returning multiple columns from UDF\n",
    "\n",
    "def split_shop_id(shop_id):\n",
    "    s, i = shop_id.split(\"_\")\n",
    "    return s, int(i) #must be cast to int, otherwise will return null\n",
    "\n",
    "split_shop_id_udf = f.udf(split_shop_id)\n",
    "\n",
    "df_udf_no_schema = shop_sales_weekly_series_df.withColumn(\"shop_id_splits\", split_shop_id_udf(f.col(\"shop_id\")))\n",
    "df_udf_no_schema.show(truncate=False)\n",
    "df_udf_no_schema.printSchema()\n",
    "\n",
    "schema = StructType([StructField(\"s\", StringType()), StructField(\"i\", IntegerType())])\n",
    "udf_with_schema = f.udf(split_shop_id, schema)\n",
    "\n",
    "df = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", udf_with_schema(f.col(\"shop_id\")))\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "#how to get each result element as separate column?\n",
    "df_split_shop_id = df.select(\"*\", \"shop_id_splits_with_schema.*\").drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id.show()\n",
    "df_split_shop_id.printSchema()\n",
    "\n",
    "#Solution above will invoke UDF as many times a there are new columns created - it's a pySpark bug fixed in Spark 2.3\n",
    "#workaround\n",
    "df_split_shop_id_correct = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", \\\n",
    "                                 f.explode(f.array(udf_with_schema(f.col(\"shop_id\")))))\n",
    "\n",
    "df_split_shop_id_correct = df_split_shop_id_correct \\\n",
    "    .select(\"*\", \"shop_id_splits_with_schema.*\") \\\n",
    "    .drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id_correct.show()\n",
    "df_split_shop_id_correct.printSchema()\n",
    "\n",
    "#TODO in Spark 2.3 we can create pandas UDFs\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
