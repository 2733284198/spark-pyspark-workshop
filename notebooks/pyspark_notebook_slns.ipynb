{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created local SparkSession\n",
      "Created \"sales\" view from CSV file\n",
      "Created \"item_prices\" view from CSV file\n"
     ]
    }
   ],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# schema of both tables\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- qty: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      "\n",
      "# sample results from both tables\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|item_id|unit_price|\n",
      "+-------+----------+\n",
      "| ITEM_1|     100.0|\n",
      "| ITEM_2|     300.0|\n",
      "| ITEM_3|      50.0|\n",
      "+-------+----------+\n",
      "\n",
      "# query execution plan for this simple select\n",
      "== Physical Plan ==\n",
      "*FileScan csv [shop_id#12,item_id#13,qty#14,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<shop_id:string,item_id:string,qty:string,transaction_date:string>\n",
      "== Physical Plan ==\n",
      "*FileScan csv [item_id#34,unit_price#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<item_id:string,unit_price:string>\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "print(\"# schema of both tables\")\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n",
    "\n",
    "print(\"# sample results from both tables\")\n",
    "sales_df.show()\n",
    "item_prices_df.show()\n",
    "\n",
    "print(\"# query execution plan for this simple select\")\n",
    "sales_df.explain()\n",
    "item_prices_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SQL support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select item_id,transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex1. Using plain SQL query select all transactions with quantity = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from sales where qty = 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex2. get mean unit price for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|avg(CAST(unit_price AS DOUBLE))|\n",
      "+-------------------------------+\n",
      "|                          150.0|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select mean(unit_price) from item_prices').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The same query as above using dataframe api\n",
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+----------------+\n",
      "|item_id|transaction_date|\n",
      "+-------+----------------+\n",
      "| ITEM_3|      2018-02-10|\n",
      "| ITEM_1|      2018-02-01|\n",
      "| ITEM_2|      2018-02-01|\n",
      "+-------+----------------+\n",
      "\n",
      "# Execution plan of a more complex query\n",
      "== Physical Plan ==\n",
      "*Sort [transaction_date#15 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(transaction_date#15 DESC NULLS LAST, 200)\n",
      "   +- *Project [item_id#13, transaction_date#15]\n",
      "      +- *Filter (isnotnull(shop_id#12) && (shop_id#12 = SHOP_1))\n",
      "         +- *FileScan csv [shop_id#12,item_id#13,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(shop_id), EqualTo(shop_id,SHOP_1)], ReadSchema: struct<shop_id:string,item_id:string,transaction_date:string>\n"
     ]
    }
   ],
   "source": [
    "shop1_transactions_df = sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\n",
    "\n",
    "print(\"# The same query as above using dataframe api\")\n",
    "shop1_transactions_df.show()\n",
    "\n",
    "\n",
    "sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\\\n",
    "    .show()\n",
    "\n",
    "print(\"# Execution plan of a more complex query\")\n",
    "\n",
    "shop1_transactions_df.explain()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex3. rewrite query from ex1 to dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.filter(f.col(\"qty\") == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# joins using plain SQL queries\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "# using Dataframe API - duplicated item_id column!\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "|shop_id|item_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1| ITEM_2|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+-------+---+----------------+-------+----------+\n",
      "\n",
      "# dropping redundant column\n",
      "+-------+---+----------------+-------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|\n",
      "+-------+---+----------------+-------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|\n",
      "+-------+---+----------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# joins using plain SQL queries\")\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "print(\"# using Dataframe API - duplicated item_id column!\")\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "print(\"# dropping redundant column\")\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex4. Filter out excluded items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dataframe with column of items we would like to exclude\n",
      "+------+\n",
      "|  item|\n",
      "+------+\n",
      "|ITEM_2|\n",
      "|ITEM_4|\n",
      "+------+\n",
      "\n",
      "# using join and filtering - result doesn't contain ITEM_2 and ITEM_4 \n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n",
      "# better option: anti join\n",
      "+-------+-------+---+----------------+\n",
      "|shop_id|item_id|qty|transaction_date|\n",
      "+-------+-------+---+----------------+\n",
      "| SHOP_1| ITEM_3|  4|      2018-02-10|\n",
      "| SHOP_2| ITEM_3|  1|      2018-02-02|\n",
      "| SHOP_1| ITEM_1|  2|      2018-02-01|\n",
      "| SHOP_2| ITEM_1|  1|      2018-02-11|\n",
      "+-------+-------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Dataframe with column of items we would like to exclude\")\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "excluded_items_df.show()\n",
    "\n",
    "print(\"# using join and filtering - result doesn't contain ITEM_2 and ITEM_4 \")\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_outer\")\\\n",
    "    .filter(f.isnull(excluded_items_df.item))\\\n",
    "    .drop(excluded_items_df.item)\\\n",
    "    .show()\n",
    "    \n",
    "print(\"# better option: anti join\")\n",
    "sales_df.join(excluded_items_df, sales_df.item_id == excluded_items_df.item, \"left_anti\")\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Added new total_sales column which is a multuply of unit_price and qty\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|\n",
      "+-------+---+----------------+-------+----------+-----------+\n",
      "\n",
      "# Adding price category column based on a condition\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|price_category|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|          High|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|          High|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|          High|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|           Low|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|        Medium|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "\n",
    "print(\"# Added new total_sales column which is a multuply of unit_price and qty\")\n",
    "total_sales_df.show()\n",
    "\n",
    "print(\"# Adding price category column based on a condition\")\n",
    "sales_with_transaction_category = total_sales_df\\\n",
    "    .withColumn(\"price_category\", \\\n",
    "                f.when(f.col(\"total_sales\") > 150, \"High\")\\\n",
    "                .when(f.col(\"total_sales\") < 60, \"Low\")\\\n",
    "                .otherwise(\"Medium\"))\n",
    "\n",
    "sales_with_transaction_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex5. We want to create two-packs of items, but their price must be lower than 360, choose those items.\n",
    "hint: use cross join, and alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|item_id|item_id|price_sum|\n",
      "+-------+-------+---------+\n",
      "| ITEM_1| ITEM_3|    150.0|\n",
      "| ITEM_2| ITEM_3|    350.0|\n",
      "| ITEM_3| ITEM_1|    150.0|\n",
      "| ITEM_3| ITEM_2|    350.0|\n",
      "+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_prices_df.alias(\"items1\")\\\n",
    "    .crossJoin(item_prices_df.alias(\"items2\"))\\\n",
    "    .withColumn(\"price_sum\",f.col(\"items1.unit_price\") + f.col(\"items2.unit_price\"))\\\n",
    "    .where((f.col(\"price_sum\") < 360) & (f.col(\"items1.item_id\") != f.col(\"items2.item_id\")))\\\n",
    "    .select(\"items1.item_id\", \"items2.item_id\", \"price_sum\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# aggregate sales by shop - ugly column name\n",
      "+-------+----------------+\n",
      "|shop_id|sum(total_sales)|\n",
      "+-------+----------------+\n",
      "| SHOP_2|           150.0|\n",
      "| SHOP_1|           700.0|\n",
      "+-------+----------------+\n",
      "\n",
      "# using alias to have a better column name\n",
      "+-------+-----+\n",
      "|shop_id|sales|\n",
      "+-------+-----+\n",
      "| SHOP_1|700.0|\n",
      "| SHOP_2|150.0|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# aggregate sales by shop - ugly column name\")\n",
    "total_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(total_sales_df.total_sales))\\\n",
    "    .orderBy(f.col(\"sum(total_sales)\")).show()\n",
    "    \n",
    "print(\"# using alias to have a better column name\")\n",
    "total_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(total_sales_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(sales_df.sales) won't work as sales_with_prices has no price column (we define it later)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex6. produce a list of all shops where each item was sold, new column should be named \"shops\"\n",
    "hint: collect_list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|item_id|           shops|\n",
      "+-------+----------------+\n",
      "| ITEM_3|[SHOP_1, SHOP_2]|\n",
      "| ITEM_2|        [SHOP_1]|\n",
      "| ITEM_1|[SHOP_1, SHOP_2]|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_df\\\n",
    "    .groupBy(\"item_id\")\\\n",
    "    .agg(f.collect_list(f.col(\"shop_id\")).alias(\"shops\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Date handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# extracting multiple elements of date\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|year|month|day|day_of_year|day_of_week|day_of_week_string|week_of_year|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018|    2|  1|         32|          4|               Thu|           5|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018|    2| 10|         41|          6|               Sat|           6|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018|    2|  2|         33|          5|               Fri|           5|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018|    2| 11|         42|          7|               Sun|           6|\n",
      "+-------+---+----------------+-------+----------+-----------+----+-----+---+-----------+-----------+------------------+------------+\n",
      "\n",
      "# aggregate sales by week\n",
      "+----------------------------+----------------+\n",
      "|weekofyear(transaction_date)|sum(total_sales)|\n",
      "+----------------------------+----------------+\n",
      "|                           6|           300.0|\n",
      "|                           5|           550.0|\n",
      "+----------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# extracting multiple elements of date\")\n",
    "total_sales_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_string\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()\n",
    "    \n",
    "\n",
    "print(\"# aggregate sales by week\")\n",
    "total_sales_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex7. Weekly sales aggregation not starting on Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------+\n",
      "|weekofyear(transaction_date_moved)|sum(total_sales)|\n",
      "+----------------------------------+----------------+\n",
      "|                                 6|           200.0|\n",
      "|                                 5|           550.0|\n",
      "|                                 7|           100.0|\n",
      "+----------------------------------+----------------+\n",
      "\n",
      "+----------+----------------+------------------+\n",
      "| aggr_date|sum(total_sales)|day_of_week_string|\n",
      "+----------+----------------+------------------+\n",
      "|2018-02-10|           200.0|               Sat|\n",
      "|2018-02-17|           100.0|               Sat|\n",
      "|2018-02-03|           550.0|               Sat|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_df\\\n",
    "    .withColumn(\"transaction_date_moved\", f.date_add(f.col(\"transaction_date\"), 1))\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date_moved\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()\n",
    "\n",
    "# Unfortunately week_of_year column will have incorrect values (shifted by 1 day)\n",
    "# but that's not a problem for calculations that require only ordering\n",
    "\n",
    "# Different solution where we preserve last day of every week \n",
    "#\"Sat\" can be seen as a day where week ends\n",
    "total_sales_df\\\n",
    "    .withColumn(\"aggr_date\", f.next_day(f.date_sub(f.col(\"transaction_date\"), 1), \"Sat\"))\\\n",
    "    .groupBy(f.col(\"aggr_date\"))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .withColumn(\"day_of_week_string\",  f.date_format(f.col(\"aggr_date\"), 'E'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using results of one query in another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Calculate global max date\n",
      "+----------+\n",
      "|  max_date|\n",
      "+----------+\n",
      "|2018-02-11|\n",
      "+----------+\n",
      "\n",
      "# Let's add it to every column using collect - calling an action\n",
      "2018-02-11\n",
      "# adding it as a literal (constant) column\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|global_max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|     2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|     2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|     2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|     2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Calculate global max date\")\n",
    "total_sales_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .show()\n",
    "    \n",
    "print(\"# Let's add it to every column using collect - calling an action\")\n",
    "# 1. using collect/first\n",
    "max_date = total_sales_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\\\n",
    "    .first()[0] #first returns first row, collect returns list of rows\n",
    "    #.collect()[0][0]\n",
    "\n",
    "print(max_date)\n",
    "    \n",
    "print(\"# adding it as a literal (constant) column\")\n",
    "sales_with_max_global_date_df = total_sales_df\\\n",
    "    .withColumn(\"global_max_date\", f.lit(max_date))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex8. using crossJoin (doesn't require invoking action - collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|  max_date|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|2018-02-11|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|2018-02-11|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------+\n",
      "\n",
      "# make sure DF inside cross join has only one element, if not then we'll have too many rows\n"
     ]
    }
   ],
   "source": [
    "max_date_df = total_sales_df\\\n",
    "    .select(f.max(f.col(\"transaction_date\")).alias(\"max_date\"))\n",
    "    \n",
    "sales_with_max_global_date_cross_join_df = total_sales_df\\\n",
    "    .crossJoin(f.broadcast(max_date_df))\\\n",
    "    .show()\n",
    "print(\"# make sure DF inside cross join has only one element, if not then we'll have too many rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# get max transaction date for each shop using simple aggregations\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "# careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings.There's no need to drop column\n",
      "# another option is to use Windows\n",
      "# Note: Windows are experimental feature (even though they're available since Spark 1.4)\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|max_transaction_date_by_shop|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  2018-02-11|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                  2018-02-11|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  2018-02-10|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  2018-02-10|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                  2018-02-10|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------------+\n",
      "\n",
      "# Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|item_transaction_ordinal|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                       1|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                       2|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                       1|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                       1|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                       2|\n",
      "+-------+---+----------------+-------+----------+-----------+------------------------+\n",
      "\n",
      "# Find average of prices from last two transactions in given shop ordered by transaction date\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|price_moving_average|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|               250.0|\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|               200.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|               250.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# get max transaction date for each shop using simple aggregations\")\n",
    "max_date_by_store_df = total_sales_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "total_sales_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show()\n",
    "print('# careful: \"shop_id\" in join is not column - just a string. Can be also a list of strings.\\\n",
    "There\\'s no need to drop column')\n",
    "\n",
    "print(\"# another option is to use Windows\")\n",
    "print(\"# Note: Windows are experimental feature (even though they're available since Spark 1.4)\")\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n",
    "    \n",
    "print(\"# Find ordinals for transactions for each item_id (so the oldest transaction with given item_id should be 1)\")\n",
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()\n",
    "    \n",
    "print(\"# Find average of prices from last two transactions in given shop ordered by transaction date\")\n",
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex9. Find average of prices from current and all previous transactions in given shop ordered by transaction date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|average_price_until_now|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                  200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                  250.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|     233.33333333333334|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                   50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                   75.0|\n",
      "+-------+---+----------------+-------+----------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unbounded_window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "    \n",
    "total_sales_df\\\n",
    "    .withColumn(\"average_price_until_now\", f.mean(f.col(\"total_sales\")).over(unbounded_window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Complex aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# produce weekly sales: one row per shop and a list of all transactions with week and year numbers for given store in one column \n",
      "# adding week and year columnns\n",
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "# aggregating sales with three collect_list invocations\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|shop_id|collect_list(week)|collect_list(year)|collect_list(sales)|\n",
      "+-------+------------------+------------------+-------------------+\n",
      "|SHOP_2 |[5, 6]            |[2018, 2018]      |[50.0, 100.0]      |\n",
      "|SHOP_1 |[5, 6]            |[2018, 2018]      |[500.0, 200.0]     |\n",
      "+-------+------------------+------------------+-------------------+\n",
      "\n",
      "# Solution above won't work as ordering in each column may be different\n",
      "# Using struct inside collect_list solves the problem\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n",
      "# What about sorting?\n",
      "# we could do it before aggregation:\n",
      "+-------+----+----+-----+\n",
      "|shop_id|week|year|sales|\n",
      "+-------+----+----+-----+\n",
      "| SHOP_1|   5|2018|500.0|\n",
      "| SHOP_1|   6|2018|200.0|\n",
      "| SHOP_2|   5|2018| 50.0|\n",
      "| SHOP_2|   6|2018|100.0|\n",
      "+-------+----+----+-----+\n",
      "\n",
      "# And then use collect_list aggregation\n",
      "+-------+--------------------------------+\n",
      "|shop_id|sales_ts                        |\n",
      "+-------+--------------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|\n",
      "+-------+--------------------------------+\n",
      "\n",
      "# But it won't work, because collect_list may not preserve ordering!\n",
      "# We need to sort it for every row - and to do that we need UDFs - User Defined Functions\n"
     ]
    }
   ],
   "source": [
    "print(\"# produce weekly sales: one row per shop and a list of all transactions \\\n",
    "with week and year numbers for given store in one column \")\n",
    "\n",
    "weekly_sales_by_shop_df = total_sales_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "\n",
    "print(\"# adding week and year columnns\")\n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "print(\"# aggregating sales with three collect_list invocations\")\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "print(\"# Solution above won't work as ordering in each column may be different\")\n",
    "    \n",
    "# shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "#     .groupBy(\"shop_id\")\\\n",
    "#     .agg(f.collect_list([\"sales\", \"week\"]))\n",
    "# won't work, can't collect more than one column\n",
    "\n",
    "print(\"# Using struct inside collect_list solves the problem\")\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "\n",
    "print(\"# What about sorting?\")\n",
    "print(\"# we could do it before aggregation:\")\n",
    "\n",
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "  \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "print(\"# And then use collect_list aggregation\")\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)\n",
    "print(\"# But it won't work, because collect_list may not preserve ordering!\")\n",
    "\n",
    "print(\"# We need to sort it for every row - and to do that we need UDFs - User Defined Functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Defining custom UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Adding new column by appending string to another one\n",
      "+-------+--------------------+--------------------+\n",
      "|shop_id|            sales_ts|  sales_ts_after_udf|\n",
      "+-------+--------------------+--------------------+\n",
      "| SHOP_2|[[2018,5,50.0], [...|AFTER_UDF_[Row(ye...|\n",
      "| SHOP_1|[[2018,5,500.0], ...|AFTER_UDF_[Row(ye...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n",
      "# Schema of the new dataframe\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- sales_ts_after_udf: string (nullable = true)\n",
      "\n",
      "# We can register our UDF in catalog and use it in SQL query\n",
      "+----------------+\n",
      "| my_udf(shop_id)|\n",
      "+----------------+\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_1|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "|AFTER_UDF_SHOP_2|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_custom_function(column1):\n",
    "    return \"AFTER_UDF_\" + str(column1)\n",
    "\n",
    "my_custom_udf = f.udf(my_custom_function)\n",
    "print(\"# Adding new column by appending string to another one\")\n",
    "df_after_udf = shop_sales_weekly_series_df.withColumn(\"sales_ts_after_udf\", my_custom_udf(f.col(\"sales_ts\")))\n",
    "df_after_udf.show()\n",
    "print(\"# Schema of the new dataframe\")\n",
    "df_after_udf.printSchema()\n",
    "\n",
    "print(\"# We can register our UDF in catalog and use it in SQL query\")\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext.registerFunction(\"my_udf\", my_custom_function)\n",
    "\n",
    "spark.sql(\"select my_udf(shop_id) from sales\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex10. Create your own UDF calculating sales for given transaction by multiplying qty and unit_price  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------------+-------+----------+-----------+----------------------+\n",
      "|shop_id|qty|transaction_date|item_id|unit_price|total_sales|custom_udf_total_sales|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------+\n",
      "| SHOP_1|  2|      2018-02-01| ITEM_1|     100.0|      200.0|                 200.0|\n",
      "| SHOP_1|  1|      2018-02-01| ITEM_2|     300.0|      300.0|                 300.0|\n",
      "| SHOP_1|  4|      2018-02-10| ITEM_3|      50.0|      200.0|                 200.0|\n",
      "| SHOP_2|  1|      2018-02-02| ITEM_3|      50.0|       50.0|                  50.0|\n",
      "| SHOP_2|  1|      2018-02-11| ITEM_1|     100.0|      100.0|                 100.0|\n",
      "+-------+---+----------------+-------+----------+-----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_custom_multiply(col1, col2):\n",
    "    return col1 * col2\n",
    "\n",
    "my_custom_multiply_udf = f.udf(my_custom_multiply)\n",
    "\n",
    "total_sales_df.withColumn(\"custom_udf_total_sales\", my_custom_multiply(f.col(\"qty\"), f.col(\"unit_price\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Returning more than one value from UDF without providing result schema\n",
      "# Results not as expected - seems like calling toString on object\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |\n",
      "+-------+--------------------------------+----------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@2c6b5f2b|\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@ea02eb8 |\n",
      "+-------+--------------------------------+----------------------------+\n",
      "\n",
      "# Actual inferred schema: one string instead of a tuple\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      "\n",
      "# Defining correct schema with two fields\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|shop_id|sales_ts                        |shop_id_splits              |shop_id_splits_with_schema|\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "|SHOP_2 |[[2018,5,50.0], [2018,6,100.0]] |[Ljava.lang.Object;@573d5bf8|[SHOP,2]                  |\n",
      "|SHOP_1 |[[2018,5,500.0], [2018,6,200.0]]|[Ljava.lang.Object;@34671d60|[SHOP,1]                  |\n",
      "+-------+--------------------------------+----------------------------+--------------------------+\n",
      "\n",
      "# Actual schema is correct as well\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- shop_id_splits_with_schema: struct (nullable = true)\n",
      " |    |-- s: string (nullable = true)\n",
      " |    |-- i: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, ArrayType, StructField\n",
    "\n",
    "\n",
    "print(\"# Returning more than one value from UDF without providing result schema\")\n",
    "def split_shop_id(shop_id):\n",
    "    s, i = shop_id.split(\"_\")\n",
    "    return s, int(i) #must be cast to int, otherwise will return null\n",
    "\n",
    "split_shop_id_udf = f.udf(split_shop_id)\n",
    "df_udf_no_schema = shop_sales_weekly_series_df.withColumn(\"shop_id_splits\", split_shop_id_udf(f.col(\"shop_id\")))\n",
    "print(\"# Results not as expected - seems like calling toString on object\")\n",
    "df_udf_no_schema.show(truncate=False)\n",
    "\n",
    "print(\"# Actual inferred schema: one string instead of a tuple\")\n",
    "df_udf_no_schema.printSchema()\n",
    "\n",
    "print(\"# Defining correct schema with two fields\")\n",
    "schema = StructType([StructField(\"s\", StringType()), StructField(\"i\", IntegerType())])\n",
    "udf_with_schema = f.udf(split_shop_id, schema)\n",
    "\n",
    "df = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", udf_with_schema(f.col(\"shop_id\")))\n",
    "df.show(truncate=False)\n",
    "print(\"# Actual schema is correct as well\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating multiple columns based on a result from UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Extracting all fields from returned struct can be done using asterisk *\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "# Schema was updated and new fields have correct types\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n",
      "# Solution above will invoke UDF as many times a there are new columns created - it's a pySpark behaviour https://issues.apache.org/jira/browse/SPARK-17728\n",
      "# for costly UDF (and in pySpark most of them are very costly) we have a workaround to explode an array with one element - result of the UDF\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "|shop_id|            sales_ts|      shop_id_splits|   s|  i|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[Ljava.lang.Objec...|SHOP|  2|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[Ljava.lang.Objec...|SHOP|  1|\n",
      "+-------+--------------------+--------------------+----+---+\n",
      "\n",
      "# Results and schema are the same\n",
      "root\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- sales_ts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- week: integer (nullable = true)\n",
      " |    |    |-- sales: double (nullable = true)\n",
      " |-- shop_id_splits: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- i: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Extracting all fields from returned struct can be done using asterisk *\")\n",
    "df_split_shop_id = df.select(\"*\", \"shop_id_splits_with_schema.*\").drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id.show()\n",
    "print(\"# Schema was updated and new fields have correct types\")\n",
    "df_split_shop_id.printSchema()\n",
    "\n",
    "print(\"# Solution above will invoke UDF as many times a there are new columns created - \\\n",
    "it's a pySpark behaviour https://issues.apache.org/jira/browse/SPARK-17728\")\n",
    "print(\"# for costly UDF (and in pySpark most of them are very costly) we have a workaround \\\n",
    "to explode an array with one element - result of the UDF\")\n",
    "df_split_shop_id_correct = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", \\\n",
    "                                 f.explode(f.array(udf_with_schema(f.col(\"shop_id\")))))\n",
    "\n",
    "df_split_shop_id_correct = df_split_shop_id_correct \\\n",
    "    .select(\"*\", \"shop_id_splits_with_schema.*\") \\\n",
    "    .drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id_correct.show()\n",
    "print(\"# Results and schema are the same\")\n",
    "df_split_shop_id_correct.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Identifying problems with UDFs\n",
      "# To see why Spark invokes UDF multiple times let's look at query execution plan\n",
      "# For the first version we can see: \n",
      "# +- BatchEvalPython [split_shop_id(shop_id#238), split_shop_id(shop_id#238), split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, pythonUDF0#5025, pythonUDF1#5026, pythonUDF2#5027]\n",
      "# which contains multiple pythonUDF references\n",
      "\n",
      "# For the updated solution there's only one invocation: \n",
      "# +- BatchEvalPython [split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, shop_id_splits#4556, pythonUDF0#5031]\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [shop_id#12, sales_ts#2049, pythonUDF0#2441 AS shop_id_splits#2230, pythonUDF2#2443.s AS s#2319, pythonUDF2#2443.i AS i#2320]\n",
      "+- BatchEvalPython [split_shop_id(shop_id#12), split_shop_id(shop_id#12), split_shop_id(shop_id#12)], [shop_id#12, sales_ts#2049, pythonUDF0#2441, pythonUDF1#2442, pythonUDF2#2443]\n",
      "   +- ObjectHashAggregate(keys=[shop_id#12], functions=[collect_list(named_struct(year, year#1904, week, week#1903, sales, sales#1913), 0, 0)])\n",
      "      +- Exchange hashpartitioning(shop_id#12, 200)\n",
      "         +- ObjectHashAggregate(keys=[shop_id#12], functions=[partial_collect_list(named_struct(year, year#1904, week, week#1903, sales, sales#1913), 0, 0)])\n",
      "            +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date))#2439, year(cast(transaction_date#15 as date))#2440], functions=[sum(total_sales#302)])\n",
      "               +- Exchange hashpartitioning(shop_id#12, weekofyear(cast(transaction_date#15 as date))#2439, year(cast(transaction_date#15 as date))#2440, 200)\n",
      "                  +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date)) AS weekofyear(cast(transaction_date#15 as date))#2439, year(cast(transaction_date#15 as date)) AS year(cast(transaction_date#15 as date))#2440], functions=[partial_sum(total_sales#302)])\n",
      "                     +- *Project [shop_id#12, transaction_date#15, (cast(qty#14 as double) * cast(unit_price#35 as double)) AS total_sales#302]\n",
      "                        +- *BroadcastHashJoin [item_id#13], [item_id#34], Inner, BuildRight\n",
      "                           :- *Project [shop_id#12, item_id#13, qty#14, transaction_date#15]\n",
      "                           :  +- *Filter isnotnull(item_id#13)\n",
      "                           :     +- *FileScan csv [shop_id#12,item_id#13,qty#14,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<shop_id:string,item_id:string,qty:string,transaction_date:string>\n",
      "                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                              +- *Project [item_id#34, unit_price#35]\n",
      "                                 +- *Filter isnotnull(item_id#34)\n",
      "                                    +- *FileScan csv [item_id#34,unit_price#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<item_id:string,unit_price:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [shop_id#12, sales_ts#2049, shop_id_splits#2230, shop_id_splits_with_schema#2377.s AS s#2383, shop_id_splits_with_schema#2377.i AS i#2384]\n",
      "+- *Project [shop_id#12, sales_ts#2049, shop_id_splits#2230, shop_id_splits_with_schema#2377]\n",
      "   +- Generate explode(array(pythonUDF0#2447)), true, false, [shop_id_splits_with_schema#2377]\n",
      "      +- BatchEvalPython [split_shop_id(shop_id#12)], [shop_id#12, sales_ts#2049, shop_id_splits#2230, pythonUDF0#2447]\n",
      "         +- *Project [shop_id#12, sales_ts#2049, pythonUDF0#2446 AS shop_id_splits#2230]\n",
      "            +- BatchEvalPython [split_shop_id(shop_id#12)], [shop_id#12, sales_ts#2049, pythonUDF0#2446]\n",
      "               +- ObjectHashAggregate(keys=[shop_id#12], functions=[collect_list(named_struct(year, year#1904, week, week#1903, sales, sales#1913), 0, 0)])\n",
      "                  +- Exchange hashpartitioning(shop_id#12, 200)\n",
      "                     +- ObjectHashAggregate(keys=[shop_id#12], functions=[partial_collect_list(named_struct(year, year#1904, week, week#1903, sales, sales#1913), 0, 0)])\n",
      "                        +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date))#2444, year(cast(transaction_date#15 as date))#2445], functions=[sum(total_sales#302)])\n",
      "                           +- Exchange hashpartitioning(shop_id#12, weekofyear(cast(transaction_date#15 as date))#2444, year(cast(transaction_date#15 as date))#2445, 200)\n",
      "                              +- *HashAggregate(keys=[shop_id#12, weekofyear(cast(transaction_date#15 as date)) AS weekofyear(cast(transaction_date#15 as date))#2444, year(cast(transaction_date#15 as date)) AS year(cast(transaction_date#15 as date))#2445], functions=[partial_sum(total_sales#302)])\n",
      "                                 +- *Project [shop_id#12, transaction_date#15, (cast(qty#14 as double) * cast(unit_price#35 as double)) AS total_sales#302]\n",
      "                                    +- *BroadcastHashJoin [item_id#13], [item_id#34], Inner, BuildRight\n",
      "                                       :- *Project [shop_id#12, item_id#13, qty#14, transaction_date#15]\n",
      "                                       :  +- *Filter isnotnull(item_id#13)\n",
      "                                       :     +- *FileScan csv [shop_id#12,item_id#13,qty#14,transaction_date#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/sales.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<shop_id:string,item_id:string,qty:string,transaction_date:string>\n",
      "                                       +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                                          +- *Project [item_id#34, unit_price#35]\n",
      "                                             +- *Filter isnotnull(item_id#34)\n",
      "                                                +- *FileScan csv [item_id#34,unit_price#35] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/mkromka/dev/trainings/pySpark_workshop/data/item_prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<item_id:string,unit_price:string>\n"
     ]
    }
   ],
   "source": [
    "print(\"# Identifying problems with UDFs\")\n",
    "    \n",
    "print(\"# To see why Spark invokes UDF multiple times let's look at query execution plan\")\n",
    "print(\"# For the first version we can see: \")\n",
    "print(\"# +- BatchEvalPython [split_shop_id(shop_id#238), split_shop_id(shop_id#238), split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, pythonUDF0#5025, pythonUDF1#5026, pythonUDF2#5027]\")\n",
    "print(\"# which contains multiple pythonUDF references\")\n",
    "print(\"\")\n",
    "print(\"# For the updated solution there's only one invocation: \")\n",
    "print(\"# +- BatchEvalPython [split_shop_id(shop_id#238)], [shop_id#238, sales_ts#4212, shop_id_splits#4556, pythonUDF0#5031]\")\n",
    "print(\"\")\n",
    "df_split_shop_id.explain()\n",
    "print(\"\")\n",
    "df_split_shop_id_correct.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.11 sort each time series from previous part in descending order and compare to initial ts (tip: use sorted method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|shop_id|            sales_ts|           sorted_ts|\n",
      "+-------+--------------------+--------------------+\n",
      "| SHOP_2|[[2018,5,50.0], [...|[[2018,6,100.0], ...|\n",
      "| SHOP_1|[[2018,5,500.0], ...|[[2018,6,200.0], ...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "def sort_ts(ts):\n",
    "    s_ts = sorted(ts, key=lambda row: (-row.week, -row.year))\n",
    "    return s_ts\n",
    "\n",
    "sort_ts_udf = f.udf(sort_ts, ArrayType(StructType(\n",
    "            [StructField(\"year\", IntegerType()),\n",
    "             StructField(\"week\", IntegerType()),\n",
    "             StructField(\"sales\", FloatType())])))\n",
    "\n",
    "sorted_ts_df = wrongly_sorted_series_df.withColumn(\"sorted_ts\", sort_ts_udf(f.col(\"sales_ts\")))\n",
    "\n",
    "sorted_ts_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
